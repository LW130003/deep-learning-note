# Stanford Seminar - Information Theory of Deep Learning
https://www.youtube.com/watch?v=XL07WEc2TRI

Last watched: 28:28

Two themes: Information Theory and Deep Neural Network Learning

Deep Learning: Neural-Netss Strike Back

The idea to put linear rectifier, etc. in networks with many layers is quite old. Tender the notion perceptron or multi-layer perceptron. The idea of actually putting many layers of these non linear special function as some sort of a pattern recognition system. Nobody was talking about learning then but is there but this was ruled out as not practical in the standards of 60s and 70s having too many parameters highly nonlinear, impossible to train because you're going to get roungh in local optima. 

All the good stories that we all know how to tell our students that too many parameters it's not going to work and it was essentially ruled out in a very in avery bright brilliant by a remote area of Minsky and Papert two really important names in the story of history I and who gave us all day completely rigorously by the way all the good reasons why it will never work.

Buttnik killed the idea in the 80s by introducing the kernel methods and support vector machines which are some sort of neural networks but simpler and easier to analyze and proof. For about decade they dominat the machine learnings in the 90s. During the late eighties by the way some physicists including me started to do something which we called statistical physics of neural network which is essentially analyzing large scale diamond in the thermodynamic limits in some sense networks like this but what we could do is really show off our mathematical techniques on very simple toy problems like one layer perceptron in some cases two layers this three layers system we could not see anything interesting using these methods and of course the whole idea of using this large-scale statistical physics went wasn't essentially didn't leave a mark I mean it was unnoticed after if some physicsist still like it but it's didn't make any big difference on the machine learning community at least but then during the last decade and since the late 2000 pushed the idea of neural network what we called deep neural networks by adding many many layers and the number and of course it's not only the number of layers but the size of the problems (inputs). We moved from inputs size of hundreds, pixels to mega pixels. Deep Neural Network started to beat all those competition, signal processing, speech recognition, etc. essentially they took over everything we called AI. Inspired by the brains and various way, but the resemblance is very weak at best.

## We begin to obtain a real theory

We combine 3 different ingredients:
- Rethinking Statistical Learning Theory
	- New: PAC-like generalization bound - with a twist ...
	- From expressivity/Hypothesis class -> Input compression bounds
- Information Theory (Statistical Mechanics...)
	- Large scale learning - Typical input patterns
	- -> Huge parameter space - exponentially many optimal solutions
- Stochastic dynamic of training process
	- Convergence of SGD to locally-Gibbs (Max Entropy) weight distribution
	- -> THe mechanism of representation compression in deep learning
	- -> Convergence times - explains the benefit of the hidden layers.

During the last 5 years essentially, we started to explore the connection a the information theoretic way of extracting relevant small dimensional relevant variables from large complex data which later in the 90s we call the information button like matter I come back to this and I had a very strong feeling that this is the right theory or the right way of understanding why this deep neural networks work and of course this was largely ignored as well for a while but then we start to do some experiments and some simulations in ourselves to give some talks which are on Youtube. Everybody somehow see this picture and many invites to talk are coming. Very nice and a big surprise

What is really there this story? We combine 3 different things which in a sense change the paradigm of learning theory. So in learning theory what I call rethinking learning theory, move from what we used to call distribution independent bound of the type of the probably approximately correct model of valiant which essentially gives you bound on generalization error which are independent of the distribution which means independent of the problem but are very strongly dependent on the architecture or what we usually call the hypothesis class.

I mean what type of functions my machine or my algorithm can generate. So, we move from these to a different type of bounds which are actually strongly sensitive to the problem, the problem dependent their distribution dependent, but they're much less sensitive to the architecture. So I moved the universality from the problem of the distributional problem to the architecture of the slam chased by time. So from worst case distribution dependent results we moved to typical case distribution dependent but architecture independent result. And I believe that this is very important for this type of machines the worst-case analysis doesn't seem to get us anywhere. Yes I do by actually using all the ideas that are just rethinking information theory of Shannon's communication theory in a slightly different way. Using the same techniques or the same mathematics that is used in information theory which is the notion of typical sequences or typicality of very large patterns I'm reusing it but the typicality here is not in time but actually let's say in the patches of the images that we train or in pieces of speech or whatever the signal is I take it inot account that it's really big now because it's really big we can use those typical average arguments and actually get very precise results. So this is where Information theory comes into the game. 

What information theory is not going to tell us much about the most important aspect of this question of deep neural networks which is that the fact that we can actually can train it effectively train it in finite time. I mean in hours or days or weeks but not in tens of years on our computers today which means that there's something that scales and very nicely with the size of the problem without paying financially long times. Time is a different issue which is really the computationally aspects and it's not going to be answered by information theoretic ideas because information measures our environment to computational complexity in a very profound sense so way what come then the third ingredient here is really these stochastic dynamics of the training algorithms, which is what we call stochastic gradient descent or backpropagation error. These are things that we are using for deep neural networks and those algorithms turn out to be not so stupid as we use to thing. I mean this is exactly what Minsky and Papert told us is never going to work actually they work miraculously well and any attempt to improve them let's say from by moving to things like second-order methods or methods which are not just now sliding a lot of the gradient or in a noisy version of the gradient tend to usually don't work well or actually don't work at all with this type of large problems. So the question is why?

So I'm actually going to connect all this three things. The idea is simple and I just want to establish a few facts.

Deep Neural Nets and Information Theory
Neural networks picture.
- Each of these circles are neurons which means a linear thereshold gate which and now think about the input layer, let's say pixel of an image as a very high entropy variable, a very high dimensional variable which I call X and think about the labels and I'm not talking about the simplest form of deep learning which is the supervised learning. I'm showing the images and give. The labels , the desired label is very simple variable which can be only just one bit. A lot simpler than X. The problem is that this one bit is not encoded in any simple way in the image there's no one pixel or one bit in the image that tells me this it's highly distributed if that's the so what actually happens in neural network is that this input or this first layer is now going through a transformation of layers and each one of them is a new representation of my image and just by thinking looking at it you see that there is a Markov chain of representation here so each representation can be calculate only from the previous one and affects only the next one so the story I want to understand is what's actually going on through this a representation transformation of this cascade of representation changes now before I go I must

## Some information Theory Basics

- THe KL distribution divergence: for any two distribution p(x) and q(x) over X: D[p(x)||q(x)] = sigma_x p(x) log(p(x)/q(x)) >= 0
- The Mutual InformationL  for any two random variablex, X, Y: I(X,Y) = D[p(x,y)||p(x)p(y)]  = D[p(x|y)||p(x)] = D[p(y|x)||p(y)] = H(X) - H(X|Y)
- Data processing Inequality (DPI) & Invariance:
	for any Markov chain: X->Y->Z: I(X;Y) >= I(X;Z)
	Reparameterization Invariance, for invertible phi, psi; I(X;Y) = I(phi(X); psi(Y))

Notion of Mutual Information. Two important quantities Kullback-Leibler Divergence and Mutual Information. Fundamental quantity and very information. 

Know statistical hypothesis testing for example know that the log like ratio is very important things and the average of it is really telling us a lot about the rate of achieving making decisions and so many other things for the has entirely different ways and meanings information theory

Mutual Information - KL divergence between the joint distribution of two variables and the product of the marginals and it's going to be zero when two variables are independent the joint is the product and then and otherwise it's a non-negative quantity which can be interpreted as the entropy or the uncertainty in the variable X - uncertainty in variable X given Y. This is a symmetric quantity and it's actually going to be very important for us because I'm actually going to from now hold on I'm going to look at mutual information quantity in my neural networks all over the place.

Now there are two things about mutual information that I want you to appreciate one of them is known as the data processing inequality which means that If I'm moving along a Markov Chain. Let's say X -> Y -> Z, in this case and then information can only decrease this is true not only finished information so for a whole range of other measures like this but there's something very special about new information which is uniquely determined not only the person quality has another property which I need which we usually call the mean as minimizer. I'm going to need it about when I'm going to talk about now.

So first an immediate consequence of the DPR data Processing Inequality is that under any invertible maps of X and Y the information doesn't change so this is actually the bad news for anybody was trying to use information for computational complexity because it means that I can increase my I can take any hard transformation how invert transformation of my data this is not going to affect information measures but it's going to create a big headache in terms of computational complexity so information is not telling us anything about computational complexity but now I look at it my neural 

Data Processing Inequality
H(X) >= I(X;hi) >= I(X;hi+1) >= ...
I(X;Y) >= I(hi;Y) >= I(hi+1;Y) >= ...

But now I look at it my neural networks there is immediately a chain of inequality which I want to call the information path which is essentially how much information is there at layer one about the input. How much information is there in layer 2 about the input. And because it is a Markov Chain of transformation, information can only decrease about the input and If I ask the same about the desired output, remember I am talking about generalization about what the label should be not what the label is at the end of the network and this is also decreasing when I go through the layers. So I have these twon chain of inequality which I'm going to call the information path of a network.

Essentially, what you should appreciate Markov chain of each layer is inducing some sort of partitioning which of the data. So think about all the images that are met to the same value of the alyer the same representation of the units and this partition can only get closer when I move from one layer to the next. So there's some sort of questioning of the representation. So this is an information theory this is related to something we call successful refinement (I'm going to come back to this at the end) And notice that of course I or the network can scramble my representation in an arbitrary way and this will not affect those information regions.

- A Markov Chain of topologically distinct [soft] partitions of the input variable X
- Successive Refinement of Relevant Information
- Individual neurons canbe "scrambled" within each layer

Another slighlty different way of thinking about it is to think about each layer as being encoded by some sort so there's a meta stochastic map in general from the input to the layer here I call it T. Which was the same thing. So any layer T: T1, T2, .... Any of those hidden layers is actually mapped and map from the input which I call the encoder of the layer and any layer has an encoder which is how it is mapped from the input both have maps it generates and there is decoder which is the way I'm trying to extract the label of the desired level there this is the label .

During the training two things happen: 1. Try to push Yhat, output from training, to be as close as possible to Y, output real label, then I say that the network is working will and 2. When you move from one layer to the next the encoder becomes more and more complicated the first time code and the first encoder is very simple and but the decoder is very complicated. Then when the encoder become more and more complicated. The decoder become more and more simpler. At the last hidden layer the decoder is essentially just a linear social function, was just an perceptron just one layer perceptron.

Each layer is charcterized by its Encoder and Decoder Information
Theorem (Information Plane):
For typical X, the sample complexity of a DNN is completely determined by the encoder mutual information; I(X;T), of the last hidden layer; the accuracy (generalization error) is determined by the decoder Information; I(Y;T), of the last hidden layer.

The complexity of the problem shifts from the decoder to the encoder, acrros the layers.

The question is how is this miracle happens that somehow from layer to layer I have more and more information by the encoder and eventually get to a very simple decoder. And now I actually here is a very informal formulation of a theorem which we now have a much clearer formulation and I dare to say that essentially the only two numbers out of those zillion of parameters and that are really important for each layer and this is the mutual information of the encoder and mutual information of the decoder. Essentially the mutual information of the desired output given the layer. And If we know these two numbers for each layer and in particular for the last hidden layer we can predict both the two things you really care about: 1. Accuracy, the probability of making an error outside the training data, what we call the generalization error. 2. Number of samples of what we call the sample complexity.

So the trade-off within the complexity of the samples how many data I really need and what accuracy this is going to give me. It's going to depend for large enough problems where I can use typicality arguments and for large enough networks such that they can actually learn this rule. If you have to depend only on these two quantities: The information of the encoding and the information of the decoder. So this is quite remarkable because it's going to simplify the problem dramatically if you actually believe me then I don't need to know anything else about architecture (not so important). I need to know only how much information there is in the decoder and the encoder of delays. THis is what I call information plane. Sometimes we call it the two coordinates. The information about the input vs the information about the desired output. Are going to tell us a very interesting story.

Ex. 100 DNN Layers in Info-Plane without averaging

X-axis I(X;T) - the information that each of the layer has about the input
Y-axis I(Y;T) - the information that each of the layer has about the output

The colors represent layers of a specific neural networks which I trained hundred times with different random initial condition and different training data. And I change both the sample size it's the same size but different selection of the sample and the ordering of the examples because there are it's a stochastic gradient descent which I'm going to change. So other than that I am now training this deep neural networks in the most plain vanilla using tensorFLow. What we want to see is how those information values evolve in time through the epoch of training so what you see now is the initial condition of the network so right up here in blue is my first hidden layer (essentially maintained even with random initialization almost all the information about both the input and the output that's why it's hid. The first hidden layer the closest to the input)

And the last hidden layer in orange is the last layer which is supposed to learn you see that the initial condition they know very little about both the input and the output. So what happens when you start training with stochastic gradient descent. So essentially what happens. Observe the number of epoch. In the epoch of training essentially cycle all the example sin mini-batches and the way we calculate the gradient descent and in stochastic gradient descent is we don't calculate the gradient over all the examples we actually cut all the examples into small sets which we call mini batches which can be 100, 200, 300 of examples and we calculate this noisy version of the gradient and we update the weights for each one of those mini batches 

so any update of the weights through that many batches what we call epoch. So after 300 epochs which means 300 cycles of cycling of the data through my networks we get more or less to this points which is intersting. All you see the data processing inequality. You see the information goes down as you expect actually his points are almost linearly down and you see there are actually six hidden layers. 

The reason you see this clouds is because as I said we repeated the experiments hundred times with random initial condition. But up to this point around 300 epochs all the networks that the older layers went up which means they improve increase the information about the label but also to the left to the right which means they also required more information about the input. All of them all the layers together and from this points on something interesting happens it starts very very slowly moving to the left and up. And if you look at the number of epochs out of this about 10,000 epochs only 300 which is very little it took to get to this same middle point which I argue us a very important point this is where the data the training error is saturated. Now it's very small this is what we all believe the most important parts of training - fitting the labels of the data. But this happens very quickly after 300 epochs. Then the rest 10,000 - 300 = 9,700 epochs essentially slowly moving by what I'm going to call diffusion or by the noise in the gradient (there's still something that still push you down), but it's dominated by the noise in the gradient.

This is what I call two phases of learning:
1. Fitting the labels - you do this what we call empirical risk minimization  
2. Forgetting phase of the learning - which is done when the error is essentially fluctuating I mean it's not zero but it's moving up and down between those mini epochs which are subsets of training data during this phase which is not supposed to do much according to the classical theories. 

Most of the improvements in my information about the labels happen and this is strange. I mean to surprise an explanation and my story is that the gist of it is at this phase we actually forget or learn to ignore the irrelevant details of the patterns. Those things which are not labeled. the changes the differences in the background the fact that there are so many dimensions of the problem which are not important for the labels. So that's the hard part ignoring or forgetting or learning to ignore the irrelevant details of the program.

Now so the rest of the story is just trying to justify what I just told you based on some other analysis.
